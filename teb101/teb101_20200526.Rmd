---
title: "R_DataMining3"
author: "York Lin"
date: "2020年05月26日"
output: html_document
editor_options: 
  chunk_output_type: console
---

### use caret package
```{R}
#install.packages("caret")
library(modeldata)
library(caret)
data("mlc_churn")

variable.list = !names(mlc_churn) %in% c('state','area_code','account_length')
mlc_churn=mlc_churn[,variable.list]

mlc_churn = mlc_churn[,variable.list]
set.seed(2)
#把資料分成training data 和 testing data
ind<-sample(1:2, size=nrow(mlc_churn), replace=T, prob=c(0.7, 0.3))
trainset=mlc_churn[ind==1,]
testset=mlc_churn[ind==2,]

control=trainControl(method="cv", number=10,summaryFunction = multiClassSummary,classProbs=T)
#control=trainControl(method="cv", number=5)

model =train(churn~., data=trainset, method="rpart",metric='F1', trControl=control)

predictions = predict(model,testset,type='raw')
table(predictions,testset$churn)
confusionMatrix(table(predictions,testset$churn))
```

### caret 套件使用說明
```{R}
# 查詢caret package 有實作的所有演算法
names(getModelInfo())
# 查詢caret package 有沒有實作rpart演算法
names(getModelInfo())[grep('rpart',names(getModelInfo()))]
# 查詢rpart model資訊
getModelInfo('rpart')
# 查詢rpart model可以tune的parameters
getModelInfo('rpart')$rpart$parameters
```

### caret tune
```{R}

control=trainControl(method="repeatedcv", number=10, repeats=3,summaryFunction = multiClassSummary,classProbs=T)

tune_funs = expand.grid(cp=seq(0,0.1,0.01))

model =train(churn~., data=trainset, method="rpart", trControl=control,tuneGrid=tune_funs,metric="F1")

model
predictions = predict(model, testset)
confusionMatrix(table(predictions,testset$churn))
```

### svm
- https://www.youtube.com/watch?time_continue=42&v=3liCbRZPrZA&feature=emb_logo

### other algorithm
```{R}
#Logistic regression
model =train(churn~., data=trainset, method="glm",family=binomial())
predictions = predict(model, testset)
confusionMatrix(table(predictions,testset$churn))

# svm
control=trainControl(method="repeatedcv", number=10, repeats=3,summaryFunction = multiClassSummary,classProbs=T)

getModelInfo('svmLinear')$svmLinear$parameters
svm_linear_tune_funs = expand.grid(C=c(1,10,100))
svm_linear_model =train(churn~., data=trainset, method="svmLinear",trControl=control,tuneGrid=svm_linear_tune_funs)

predictions = predict(svm_linear_model, testset)
confusionMatrix(table(predictions,testset$churn))

getModelInfo('svmRadial')$svmRadial$parameters
svm_radial_tune_funs = expand.grid(C=c(1,10),sigma=c(1,10))
svm_radial_model =train(churn~., data=trainset, method="svmRadial",trControl=control,tuneGrid=svm_radial_tune_funs)

predictions = predict(svm_radial_model, testset)
confusionMatrix(table(predictions,testset$churn))
```

### find importance variable
```{R}
library('caret')
importance = varImp(model, scale=T)
importance
plot(importance)
```

### ROC
- https://www.youtube.com/watch?v=OAl6eAyP-yo
- http://www.navan.name/roc/

```{R}
#install.packages("ROCR")
library(ROCR)
pred <-predict(model, testset, type="prob")
p_yes<-pred[, "yes"]
predictions<-prediction(p_yes, testset$churn)
per_auc<-performance(predictions, measure ="auc")
per_fpr_tpr<-performance(predictions, measure="tpr",x.measure = "fpr")
plot(per_fpr_tpr,main=paste("AUC:",(per_auc@y.values)))
```


- http://www.rpubs.com/skydome20/R-Note16-Ensemble_Learning
### 補充：隨機森林(Random Forest)
```{R}
#install.packages('randomForest')
library(randomForest)
library('caret')
library('e1071')
library(ROCR)

rf_model = randomForest(formula=churn ~ .,data=trainset)
#find best ntree
plot(rf_model)
legend("topright",colnames(rf_model$err.rate),col=1:3,cex=0.8,fill=1:3)
#find nest mtry
tuneRF(trainset[,-17],trainset$churn)

rf_model <- randomForest(churn ~., data = trainset, ntree=50,mtry=4)
# rf_model = train(churn~.,data=churnTrain,method='rf')
confusionMatrix(table(predict(rf_model,testset),testset$churn))


rf_pred <-predict(rf_model, testset, type="prob")
rf_p_yes<-rf_pred[, "yes"]
rf_predictions<-prediction(rf_p_yes, testset$churn)
rf_per_fpr_tpr<-performance(rf_predictions, measure="tpr",x.measure = "fpr")
```

#### 比較CART和RandomForest
```{R}
plot(per_fpr_tpr,col='red')
plot(rf_per_fpr_tpr,col='black',add=T)
legend(x=0.7, y=0.2, legend =c('randomforest','rpart'), fill= c("black","red"))
```

# 分群問題
### 距離計算
```{R}
x =c(0, 0, 1, 1, 1, 1)
y =c(1, 0, 1, 1, 0, 1)

#euclidean
?dist
rbind(x,y)
dist(rbind(x,y), method ="euclidean")

#city block
dist(rbind(x,y), method ="manhattan")

z = c(1,1,1,0,1,1)
rbind(x,y,z)
dist(rbind(x,y,z), method ="euclidean")
dist(rbind(x,y,z), method ="manhattan")
```

## iris cluster
```{R}
# hclust
data(iris)
d = dist(iris[,-5],method = 'euclidean')
hc = hclust(d,method='ward.D2')
#hc = hclust(d,method='single')
#hc = hclust(d,method='complete')
plot(hc,cex=0.4)

fit =cutree(hc, k =3)
fit
table(fit)
plot(hc, cex=0.4)
rect.hclust(hc, k =3, border="red")

summary(iris[fit == 1,])
```

# kmeans
```{R}
set.seed(22)
iris_s = as.data.frame(scale(iris[,-5]))
k_model =kmeans(iris_s, centers=3)
?kmeans

barplot(t(k_model$centers), beside =TRUE,xlab="cluster", ylab="value")
?barplot
k_model$centers

summary(iris[k_model$cluster == 1,])
```


```{R}
library('cluster')
set.seed(22)
k_model =kmeans(iris_s, 3)
kms=silhouette(k_model$cluster,dist(iris_s))
summary(kms)
plot(kms)

## find k
#install.packages("fpc")
library(fpc)
?cluster.stats
c_stats = cluster.stats(dist(iris_s),k_model$cluster)
c_stats$avg.silwidth
c_stats$within.cluster.ss

nk=2:6
cluster.stats(dist(iris_s),kmeans(iris_s,2)$cluster)
cluster.stats(dist(iris_s),kmeans(iris_s,2)$cluster)$avg.silwidth
cluster.stats(dist(iris_s),kmeans(iris_s,2)$cluster)$within.cluster.ss

# avg silhouette
SW = sapply(nk,function(k){
  set.seed(22); cluster.stats(dist(iris_s),kmeans(iris_s,k)$cluster)$avg.silwidth
})

plot(x=nk,y=SW,type='l')

#within sum of squares
WSS =sapply(nk, function(k){set.seed(22);cluster.stats(dist(iris_s),kmeans(iris_s,k)$cluster)$within.cluster.ss})
WSS
plot(x=nk, y=WSS, type="l", xlab="number of k", ylab="within sum of squares")

```
