---
title: "R_0720"
author: "York Lin"
date: "2017年07月20日"
output: html_document
---


##package:ggplot2
documentation
- http://docs.ggplot2.org/current/

cheat sheet
- https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf

why ggplot2?
- fancy by default, good for demo and report 
- consistent across all kinds of plot in syntax and behavior
- strong support community(the mostly download package on CRAN)
```{R}
#basic syntax
#ggplot(data,aes(x,y,group,...))+geom_object(...)
install.packages('ggplot2')
library('ggplot2')
load('Statistics/cdc.Rdata')
cdc$exerany=as.factor(cdc$exerany)
cdc$hlthplan=as.factor(cdc$hlthplan)
cdc$smoke100=as.factor(cdc$smoke100)

g <- ggplot(cdc,aes(x=height,y=weight))
g+geom_point(aes(col=exerany))

g <- ggplot(cdc,aes(x=genhlth))
g+geom_bar()
g+geom_bar() + ylab('Count') + ggtitle('cdc')
#fill => 填滿顏色; color => 邊線顏色
g+geom_bar(fill='snow',color='black')

g <- ggplot(cdc,aes(x=genhlth,fill=gender))
g+geom_bar()
#g <- ggplot(cdc,aes(x=genhlth))
#g+geom_bar(aes(fill=gender))

g_bygrp <- ggplot(cdc,aes(x=exerany,fill=genhlth))
g_bygrp + geom_bar()
g_bygrp + geom_bar(position='stack')
g_bygrp + geom_bar(position='dodge')
g_bygrp + geom_bar(position='identity')

precounted = as.data.frame(table(cdc$genhlth,dnn = c('genhlth')))
precounted
ggplot(precounted,aes(x=genhlth,y=Freq))+ geom_bar(stat='identity')

g <- ggplot(cdc,aes(x=genhlth,y=height))
g + geom_boxplot()

#facet
g <- ggplot(cdc,aes(x=weight))
g2 = g+ geom_histogram()+facet_wrap(~genhlth)
g2

ggsave(filename='your_file_name.png',plot = g2)
```


##資料預處理
```{R}
getwd()
setwd('~/lecture/riii')
load('Statistics/appledaily.RData')

str(appledaily)
head(appledaily)

#把dt轉換成日期型態
#方法一：用as.POSIXct()轉換
appledaily$dt = as.POSIXct(appledaily$dt,format = '%Y年%m月%d日%H:%M')

#方法二：用strptime()轉換
strptime(appledaily$dt,'%Y年%m月%d日%H:%M')

#比較as.POSIXct() 和 as.POSIXlt
t1 = as.POSIXct(appledaily$dt,format = '%Y年%m月%d日%H:%M')
class(t1)
unclass(t1)

t2 = as.POSIXlt(appledaily$dt,format = '%Y年%m月%d日%H:%M')
class(t2)
unclass(t2)

# Date 和 POSIX 差別
# Date類別表示 "日期",  表示距離1970/1/1多少天, 單位為天
# POSIX類別表示 "時間", 表示距離1970/1/1多少秒, 單位為秒

now = Sys.time()
class(now)
unclass(now)

nowDate = as.Date(now)
class(nowDate)
unclass(nowDate)

#difftime
Sys.time() - strptime(appledaily$dt,'%Y年%m月%d日%H:%M')[1]

#擷取點擊數中數值部分
#方法一：利用sub函數取代
appledaily$clicked = sub('\\)','',sub('人氣\\(','',appledaily$clicked))
appledaily$clicked = as.integer(appledaily$clicked)
head(appledaily)

#方法二：使用stringr套件的str_match()
library(stringr)
?str_match
as.integer(str_match(appledaily$clicked,"人氣\\((\\d+)\\)")[,2])


#其他常見字串處理函式
#grep()  ==> return index
test_str = c('abcd','bcd','cde')
grep('a',test_str)
test_str[grep('a',test_str)]

#grepl() ==> return boolean 
grepl('a',test_str)
test_str[grepl('a',test_str)]

#strsplit() ==> 字串分割
strsplit('abc-def','-')
unlist(strsplit('abc-def','-'))[1]

#substring() ==> 取得部份字串
substring('abcdef',2,nchar('abcdef')-1)

names(table(appledaily$category))
appledaily$category[appledaily$category == "國際\",\"LA\",\"SF\",\"NY\",\"US"] = '國際'
appledaily$category[appledaily$category == "國際\",\"SF\",\"US"] = '國際'
names(table(appledaily$category))

#儲存處理過的檔案
applenews = appledaily
save(applenews,file = 'Statistics/applenews.RData')


#遺失值處理(missing value)
na_list = sample(1:nrow(applenews),30)
applenews[na_list,'clicked'] = NA

#找尋遺失值
is.na(applenews)
sum(is.na(applenews$clicked))

#移除missing value
complete.cases(applenews)
rm.data <- applenews[complete.cases(applenews), ]
str(rm.data)

#以全體平均填補
mean_clicked = as.integer(mean(applenews$clicked,na.rm=T))
applenews$clicked[is.na(applenews$clicked)] = mean_clicked

#以類別平均填補
cat_means = tapply(applenews$clicked,applenews$category,function(e){as.integer(mean(e,na.rm=T))})

for(i in 1:length(names(cat_means))){
  applenews[applenews$category == names(cat_means)[i] & is.na(applenews$clicked),'clicked'] = cat_means[i]
}

```

##package dplyr
- 類SQL語法,select,filter,arrange,mutate...
- Chaining %>%, debug方便

cheat sheet
- https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf

```{R}
load('Statistics/applenews.RData')
str(applenews)
head(applenews)

install.packages('dplyr')
library(dplyr)

#原先R 提供的過濾功能
applenews[applenews$category == "娛樂",]

#dplyr 的過濾功能
filter(applenews, category == "娛樂")

#and/or 
filter(applenews, category == "娛樂" & clicked > 1000)
filter(applenews, category == "娛樂" | clicked > 1000)

#篩選多個類別
filter(applenews, category %in% c("娛樂", "社會"))

#原先R的欄位選取
applenews[, c("category","clicked")]

#dplyr 的欄位選取
select(applenews,category,clicked)
select(applenews,category:clicked)
select(applenews,matches('click'))

?matches

#想同時filter 和 select
filter(select(applenews,category:clicked),category == '娛樂')

#使用Chaining
select(applenews,category:clicked) %>%
  filter(category == '娛樂')

applenews %>% 
  select(category:clicked) %>%
  filter(category == '娛樂')

#使用Arrange將資料做排序
applenews %>%
  select(category,clicked) %>% 
  filter(category == "社會") %>% 
  arrange(desc(clicked))


# 總點擊數
freqsum = applenews %>%
  select(clicked) %>% 
  sum()

#使用mutate產生新欄位
applenews %>%
  select(title, category,clicked) %>% 
  mutate(portion= clicked / freqsum) %>%
  arrange(desc(portion)) %>%
  head()

#新增portion欄位
applenews = applenews %>%
  mutate(portion= clicked / freqsum)

#group_by & summarise
applenews %>%
  group_by(category) %>%
  summarise(clicked_sum = sum(clicked, na.rm=TRUE)) %>%
  arrange(desc(clicked_sum))

#多個欄位計算
applenews %>%
  group_by(category) %>% 
  summarise_at(.vars=vars(clicked),.funs=funs(sum,mean))

applenews %>%
  group_by(category) %>%
  summarise_at(.funs=funs(min,max), .vars=vars(matches('clicked')), na.rm=T)

#一般計數
applenews %>%
  select(category) %>%
  summarise(n())

#不重複計數
applenews %>%
  select(category) %>%
  summarise(n_distinct(category))


cat_stat = applenews %>%
  group_by(category) %>%
  summarise(clicked_sum = sum(clicked)) 
#繪製長條圖
barplot(cat_stat$clicked_sum, names.arg=cat_stat$category, col=rainbow(length(cat_stat$category)))
#繪製圓餅圖
pie(cat_stat$clicked_sum, label = cat_stat$category)


#連接資料庫範例(以sqlite3為例)
# sqlite3 download page: https://www.sqlite.org/download.html
install.packages('dbplyr')
install.packages('RSQLite')
library('dbplyr')
library('RSQLite')

my_database = src_sqlite('./mydatabase',create=T)
copy_to(my_database,applenews,temporary = F)
tbl(my_database,"applenews")
tbl(my_database,"applenews") %>% collect()

category_stat = tbl(my_database,"applenews") %>% 
  group_by(category) %>%
  summarise_at(.funs=funs(min,max,mean), .vars=vars(matches('clicked'))) %>%
  arrange(desc(mean)) %>%
  collect()

library('ggplot2')
g <- ggplot(category_stat,aes(x=category,y=mean))
g + geom_bar(stat='identity') + theme(text=element_text(size=16,  family="Songti SC")) + scale_x_discrete(limits=category_stat$category)

```

##Learning map
- http://scikit-learn.org/stable/_static/ml_map.png

- http://www.r-bloggers.com/whats-the-difference-between-machine-learning-statistics-and-data-mining/

- http://mp.weixin.qq.com/s?__biz=MjM5ODczNTkwMA==&mid=2650107069&idx=1&sn=44a2eab6c4858c56af236749fdd1d784#rd

#Classification
##Decision Tree - using churn data in C50 package
```{R}
install.packages("C50")
library(C50)

data(churn)
str(churnTrain)

names(churnTrain) %in% c("state", "area_code", "account_length")
!names(churnTrain) %in% c("state", "area_code", "account_length")
#選擇建模變數
variable.list = !names(churnTrain) %in% c('state','area_code','account_length')
churnTrain=churnTrain[,variable.list]

str(churnTrain)

#sample
?sample
sample(1:10)
sample(1:10, size = 5)
sample(c(0,1), size= 10, replace = T)
sample.int(20, 12) # 兩個參數都要放整數，此例為取1:20中的12個不重複樣本


set.seed(2)
#把資料分成training data 和 testing data
ind<-sample(1:2, size=nrow(churnTrain), replace=T, prob=c(0.7, 0.3))
trainset=churnTrain[ind==1,]
testset=churnTrain[ind==2,]


table(sample(x = 1:2,size = 100, replace=T))

set.seed(1)
table(sample(x = 1:2,size = 100, replace=T, prob=c(0.7,0.3)))

a = c(1,2,3,4,5,6,7,8,9)
ind = c(1,0,1,0,1,0,1,0,1)
ind == 1
a[ind == 1]
a[ind == 0]

```

##rpart
```{R}
install.packages('rpart')
library('rpart')
#使用rpart(CART)建立決策樹模型

churn.rp<-rpart(churn ~ ., data=trainset)
churn.rp
summary(churn.rp)

con = rpart.control(cp=0.01)
?rpart.control
churn.rp<-rpart(churn ~., data=trainset,control = con)

#畫出決策樹
par(mfrow=c(1,1))
plot(churn.rp, margin=0.1)
plot(churn.rp, uniform=TRUE,branch = 0.6, margin=0.1)
?plot.rpart
text(churn.rp)
text(churn.rp, all=TRUE, use.n=TRUE)

printcp(churn.rp)
plotcp(churn.rp)
```

##Prune

```{R}
#找出minimum cross-validation errors
min(churn.rp$cptable[,"xerror"])
which.min(churn.rp$cptable[,"xerror"])
churn.cp = churn.rp$cptable[which.min(churn.rp$cptable[,"xerror"]), "CP"]
#將churn.cp設為臨界值來修剪樹
prune.tree=prune(churn.rp, cp=churn.cp)

plot(prune.tree, margin=0.1)
text(prune.tree, all=TRUE, use.n=TRUE, cex=0.7)

predictions <-predict(prune.tree, testset,type = "class")
table(testset$churn, predictions)

install.packages('caret')
install.packages('e1071')
library('caret')
library('e1071')
confusionMatrix(table(predictions, testset$churn))
?confusionMatrix

```

##ctree
```{R}
install.packages("party")
library('party')
ctree.model = ctree(churn ~ . , data = trainset)
plot(ctree.model, margin=0.1)

daycharge.model = ctree(churn ~ total_day_charge + international_plan, data = trainset)
plot(daycharge.model)

ctree.predict = predict(ctree.model ,testset)
table(ctree.predict, testset$churn)

confusionMatrix(table(ctree.predict, testset$churn))
```

##C5.0
```{R}
install.packages("C50")
library(C50)
c50.model = C5.0(churn ~., data=trainset)

?C5.0Control

c=C5.0Control(minCases = 20)
c50.model = C5.0(churn ~., data=trainset,control = c)

summary(c50.model)
plot(c50.model)

c50.predict = predict(c50.model,testset)
table(c50.predict, testset$churn)

confusionMatrix(table(c50.predict, testset$churn))
```

##Estimating model performance with k-fold cross-validation
```{R}
ind = cut(1:nrow(churnTrain), breaks=10, labels=F)
ind

accuracies = c()
for (i in 1:10) {
  fit = rpart(formula=churn ~., data=churnTrain[ind != i,])
  predictions = predict(fit, churnTrain[ind == i, ! names(churnTrain) %in% c("churn")], type="class")
  correct_count = sum(predictions == churnTrain[ind == i,c("churn")])
  accuracies = append(correct_count / nrow(churnTrain[ind == i,]), accuracies)
}
accuracies
mean(accuracies)

```

##caret cross-validation
```{R}
install.packages("caret")
library(caret)
control=trainControl(method="repeatedcv", number=10, repeats=3)
model =train(churn~., data=trainset, method="rpart", trControl=control)
model
predictions = predict(model, testset)

table(predictions,testset$churn)
```

##find importance variable
```{R}
library('caret')
importance = varImp(model, scale=FALSE)
importance
plot(importance)

```

##ROC
- https://www.youtube.com/watch?v=OAl6eAyP-yo
- http://www.navan.name/roc/

```{R}
install.packages("ROCR")
library(ROCR)
predictions <-predict(churn.rp, testset, type="prob")
head(predictions)
pred.to.roc<-predictions[, 1]
head(pred.to.roc)
pred.rocr<-prediction(pred.to.roc, testset$churn)
pred.rocr
perf.rocr<-performance(pred.rocr, measure ="auc", x.measure="cutoff")
perf.tpr.rocr<-performance(pred.rocr, "tpr","fpr")
plot(perf.tpr.rocr,colorize=T,main=paste("AUC:",(perf.rocr@y.values)))
```