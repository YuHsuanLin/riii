---
title: "R_basic5"
author: "York Lin"
date: "2019年10月15日"
output: html_document
editor_options: 
  chunk_output_type: console
---

## cp
- http://mlwiki.org/index.php/Cost-Complexity_Pruning

```{R}
library(C50)

data(churn)

names(churnTrain) %in% c("state", "area_code", "account_length")
!names(churnTrain) %in% c("state", "area_code", "account_length")
#選擇建模變數
variable.list = !names(churnTrain) %in% c('state','area_code','account_length')
churnTrain=churnTrain[,variable.list]
churnTest=churnTest[,variable.list]

set.seed(2)
#把資料分成training data 和 testing data
ind<-sample(1:2, size=nrow(churnTrain), replace=T, prob=c(0.7, 0.3))
trainset=churnTrain[ind==1,]
testset=churnTrain[ind==2,]

library('rpart')
library('caret')
library('e1071')

con = rpart.control(minsplit=20,cp=0.01)
churn.rp<-rpart(churn ~., data=trainset,control = con)
summary(churn.rp)

printcp(churn.rp)
#找出minimum cross-validation errors
min_row = which.min(churn.rp$cptable[,"xerror"])
churn.cp = churn.rp$cptable[min_row, "CP"]
#將churn.cp設為臨界值來修剪樹
prune.tree=prune(churn.rp, cp=churn.cp)
plot(prune.tree, uniform=TRUE,branch = 0.6, margin=0.1)
text(prune.tree, all=TRUE, use.n=TRUE, cex=0.7)

predictions <-predict(prune.tree, testset, type='class')
table(predictions,testset$churn)
confusionMatrix(table(predictions, testset$churn))
```

### use caret package
```{R}
#install.packages("caret")
library(caret)
control=trainControl(method="repeatedcv", number=10, repeats=3)

model =train(churn~., data=churnTrain, method="rpart", trControl=control)

predictions = predict(model,churnTest)
table(predictions,churnTest$churn)
confusionMatrix(table(predictions,churnTest$churn))
```

### caret 套件使用說明
```{R}
# 查詢caret package 有實作的所有演算法
names(getModelInfo())
# 查詢caret package 有沒有實作rpart演算法
names(getModelInfo())[grep('rpart',names(getModelInfo()))]
# 查詢rpart model資訊
getModelInfo('rpart')
# 查詢rpart model可以tune的parameters
getModelInfo('rpart')$rpart$parameters
```

### caret tune
```{R}
control=trainControl(method="repeatedcv", number=10, repeats=3,summaryFunction = multiClassSummary,classProbs=T)


tune_funs = expand.grid(cp=seq(0,0.1,0.01))


model =train(churn~., data=churnTrain, method="rpart", trControl=control,tuneGrid=tune_funs,metric='AUC')

model
predictions = predict(model, churnTest)
confusionMatrix(table(predictions,churnTest$churn))
```

### find importance variable
```{R}
library('caret')
importance = varImp(model, scale=T)
importance
plot(importance)
```

### ROC
- https://www.youtube.com/watch?v=OAl6eAyP-yo
- http://www.navan.name/roc/

```{R}
#install.packages("ROCR")
library(ROCR)
predictions <-predict(model, churnTest, type="prob")
head(predictions)
pred.to.roc<-predictions[, "yes"]
head(pred.to.roc)
pred.rocr<-prediction(pred.to.roc, churnTest$churn)
pred.rocr
perf.rocr<-performance(pred.rocr, measure ="auc")
perf.tpr.rocr<-performance(pred.rocr, measure="tpr",x.measure = "fpr")
plot(perf.tpr.rocr,main=paste("AUC:",(perf.rocr@y.values)))
```


- http://www.rpubs.com/skydome20/R-Note16-Ensemble_Learning
### 補充：隨機森林(Random Forest)
```{R}
#install.packages('randomForest')
library(randomForest)
library('caret')
library('e1071')
library(ROCR)

rf_model = randomForest(formula=churn ~ .,data=churnTrain)
#find best ntree
plot(rf_model)
legend("topright",colnames(rf_model$err.rate),col=1:3,cex=0.8,fill=1:3)
#find nest mtry
tuneRF(churnTrain[,-17],churnTrain[,17])

rf_model <- randomForest(churn ~., data = churnTrain, ntree=50,mtry=4)
# rf_model = train(churn~.,data=churnTrain,method='rf')
confusionMatrix(table(predict(rf_model,churnTest),churnTest$churn))

rf.predict.prob <- predict(rf_model, churnTest, type="prob")
rf.prediction <- prediction(rf.predict.prob[,"yes"], as.factor(churnTest$churn))
rf.auc <- performance(rf.prediction, measure = "auc")
rf.performance <- performance(rf.prediction, measure="tpr",x.measure="fpr")
plot(rf.performance)
```

#### 比較CART和RandomForest
```{R}
tune_funs = expand.grid(cp=seq(0,0.1,0.01))
rpart_model =train(churn~., data=churnTrain, method="rpart",tuneGrid=tune_funs)

rpart_prob_yes = predict(rpart_model,churnTest,type='prob')[,1]
rpart_pred.rocr = prediction(rpart_prob_yes,churnTest$churn)
rpart_perf.rocr = performance(rpart_pred.rocr,measure = 'tpr',x.measure = 'fpr')

plot(rpart_perf.rocr,col='red')
plot(rf.performance,col='black',add=T)
legend(x=0.7, y=0.2, legend =c('randomforest','rpart'), fill= c("black","red"))
```

# 分群問題
### 距離計算
```{R}
x =c(0, 0, 1, 1, 1, 1)
y =c(1, 0, 1, 1, 0, 1)

#euclidean
?dist
rbind(x,y)
dist(rbind(x,y), method ="euclidean")

#city block
dist(rbind(x,y), method ="manhattan")

z = c(1,1,1,0,1,1)
rbind(x,y,z)
dist(rbind(x,y,z), method ="euclidean")
dist(rbind(x,y,z), method ="manhattan")

```

### 聚合式(bottom-up)
```{R}
setwd('~/lecture/riii')
customer=read.csv('data/customer.csv',header=TRUE)
head(customer)
str(customer)

#數值變數作正規化
customer_s =scale(customer[,-1])
?scale

#正規化後的變數平均數為0, 標準差為1
round(mean(customer_s[,3]),3)
round(sd(customer_s[,3]),3)

?hclust
hc=hclust(dist(customer_s, method="euclidean"), method="ward.D2")
plot(hc,hang =-0.01, cex=0.7)

hc3 =hclust(dist(customer, method="euclidean"), method="single")
plot(hc3, hang =-0.01, cex=0.8)
```

### cutree
```{R}
fit =cutree(hc, k =4)
fit
table(fit)
plot(hc, hang =-0.01, cex=0.7)
rect.hclust(hc, k =4, border="red")
rect.hclust(hc, k =3, border="blue")

c_1 = customer[fit == 1,]
summary(c_1)
```

### 分裂式階層式(top-down)
```{r}
#install.packages('cluster')
library(cluster)
?diana
dv =diana(customer_s, metric ="euclidean")
summary(dv)
plot(dv)

fit2 =cutree(dv,k=4)
c_1 = customer[fit2 ==2,]
summary(c_1)
```

### k-means
```{R}
str(customer_s)
set.seed(22)
fit =kmeans(customer_s, centers=4)
?kmeans

barplot(t(fit$centers), beside =TRUE,xlab="cluster", ylab="value")
?barplot
fit$centers

customer[fit$cluster == 1,]
```

### 投影至二維空間,繪製分群結果
```{R}
plot(customer[,-1],col=fit$cluster)

#install.packages("cluster")
library(cluster)
clusplot(customer_s, fit$cluster, color=TRUE, shade=TRUE)

#了解component 成分為何
pca =princomp(customer_s)
summary(pca)
pca$loadings
```

### Evaluating model
```{R}
#silhouette
library('cluster')
par(mfrow= c(1,1))
set.seed(22)
km =kmeans(customer_s, 4)
kms=silhouette(km$cluster,dist(customer_s))
summary(kms)
plot(kms)
```

### 選擇k-means最佳k值
```{R}
nk=2:10

# avg silhouette
SW = sapply(nk,function(k){
  set.seed(22); summary(silhouette(kmeans(customer_s,centers=k)$cluster,dist(customer_s)))$avg.width
})

plot(x=nk,y=SW,type='l')

#within sum of squares
WSS =sapply(nk, function(k){set.seed(22);kmeans(customer_s, centers=k)$tot.withinss})
WSS
plot(x=nk, y=WSS, type="l", xlab="number of k", ylab="within sum of squares")
```

### model comparison
```{R}
#install.packages("fpc")
library(fpc)
single_c=hclust(dist(customer_s), method="single")
hc_single=cutree(single_c, k =4)

complete_c=hclust(dist(customer_s), method="complete")
hc_complete=cutree(complete_c, k =4)

set.seed(22)
km =kmeans(customer_s, 4)

cs=cluster.stats(dist(customer_s),km$cluster)
cs[c("within.cluster.ss","avg.silwidth")]

q =sapply(
  list(kmeans=km$cluster, 
       hc_single=hc_single, 
       hc_complete=hc_complete), function(c)cluster.stats(dist(customer_s),c)[c("within.cluster.ss","avg.silwidth")])
q
```
