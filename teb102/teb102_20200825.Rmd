---
title: "R_DataMining3"
author: "York Lin"
date: "2020年08月25日"
output: html_document
editor_options: 
  chunk_output_type: console
---

### use caret package
```{R}
#install.packages("caret")
library(modeldata)
library(caret)
data("mlc_churn")

variable.list = !names(mlc_churn) %in% c('state','area_code','account_length')
mlc_churn=mlc_churn[,variable.list]

mlc_churn = mlc_churn[,variable.list]
set.seed(2)
#把資料分成training data 和 testing data
ind<-sample(1:2, size=nrow(mlc_churn), replace=T, prob=c(0.7, 0.3))
trainset=mlc_churn[ind==1,]
testset=mlc_churn[ind==2,]

control=trainControl(method="cv", number=10,summaryFunction = twoClassSummary,classProbs=T)
#control=trainControl(method="cv", number=5)

model =train(churn~., data=trainset, method="rpart", trControl=control,metric='F1')

predictions = predict(model,testset,type='raw')
table(predictions,testset$churn)
confusionMatrix(table(predictions,testset$churn))
```

### other algorithm
```{R}
#Logistic regression
logistic_model =train(churn~., data=trainset, method="glm",family=binomial())
logistic_predictions = predict(logistic_model, testset)
confusionMatrix(table(logistic_predictions,testset$churn))

# svm
control=trainControl(method="repeatedcv", number=10, repeats=3,summaryFunction = multiClassSummary,classProbs=T)
svm_linear_model =train(churn~., data=trainset, method="svmLinear",trControl=control)
svm_predictions = predict(svm_linear_model, testset)
confusionMatrix(table(svm_predictions,testset$churn))
```

### caret 套件使用說明
```{R}
# 查詢caret package 有實作的所有演算法
names(getModelInfo())
# 查詢caret package 有沒有實作rpart演算法
names(getModelInfo())[grep('rpart',names(getModelInfo()))]
# 查詢rpart model資訊
getModelInfo('rpart')
# 查詢rpart model可以tune的parameters
getModelInfo('rpart')$rpart$parameters
```

### caret tune
```{R}

control=trainControl(method="repeatedcv", number=10, repeats=3,summaryFunction = multiClassSummary,classProbs=T)

tune_funs = expand.grid(cp=seq(0,0.1,0.01))

model =train(churn~., data=trainset, method="rpart", trControl=control,tuneGrid=tune_funs,metric="F1")

model
predictions = predict(model, testset)
confusionMatrix(table(predictions,testset$churn))
```


### find importance variable
```{R}
library('caret')
importance = varImp(model, scale=T)
importance
plot(importance)
```

### ROC
- https://www.youtube.com/watch?v=OAl6eAyP-yo
- http://www.navan.name/roc/

```{R}
#install.packages("ROCR")
library(ROCR)
pred <-predict(model, testset, type="prob")
p_yes<-pred[, "yes"]
predictions<-prediction(p_yes, testset$churn)
per_auc<-performance(predictions, measure ="auc")
per_fpr_tpr<-performance(predictions, measure="tpr",x.measure = "fpr")
plot(per_fpr_tpr,main=paste("AUC:",(per_auc@y.values)))
```


- http://www.rpubs.com/skydome20/R-Note16-Ensemble_Learning
### 補充：隨機森林(Random Forest)
```{R}
#install.packages('randomForest')
library(randomForest)
library('caret')
library('e1071')
library(ROCR)

rf_model = randomForest(formula=churn ~ .,data=trainset)
#find best ntree
plot(rf_model)
legend("topright",colnames(rf_model$err.rate),col=1:3,cex=0.8,fill=1:3)
#find nest mtry
tuneRF(trainset[,-17],trainset$churn)

rf_model <- randomForest(churn ~., data = trainset, ntree=50,mtry=4)
# rf_model = train(churn~.,data=churnTrain,method='rf')
confusionMatrix(table(predict(rf_model,testset),testset$churn))


rf_pred <-predict(rf_model, testset, type="prob")
rf_p_yes<-rf_pred[, "yes"]
rf_predictions<-prediction(rf_p_yes, testset$churn)
rf_per_fpr_tpr<-performance(rf_predictions, measure="tpr",x.measure = "fpr")
```

#### 比較CART和RandomForest
```{R}
plot(per_fpr_tpr,col='red')
plot(rf_per_fpr_tpr,col='black',add=T)
legend(x=0.7, y=0.2, legend =c('randomforest','rpart'), fill= c("black","red"))
```

# 分群問題
### 距離計算
```{R}
x =c(0, 0, 1, 1, 1, 1)
y =c(1, 0, 1, 1, 0, 1)

#euclidean
?dist
rbind(x,y)
dist(rbind(x,y), method ="euclidean")

#city block
dist(rbind(x,y), method ="manhattan")

z = c(1,1,1,0,1,1)
rbind(x,y,z)
dist(rbind(x,y,z), method ="euclidean")
dist(rbind(x,y,z), method ="manhattan")
```


## iris cluster
```{R}
# hclust
data(iris)
d = dist(iris[,-5],method = 'euclidean')
hc = hclust(d,method='ward.D2')
#hc = hclust(d,method='single')
#hc = hclust(d,method='complete')
plot(hc,cex=0.4)

fit =cutree(hc, k =3)
fit
table(fit)
plot(hc, cex=0.4)
rect.hclust(hc, k =3, border="red")

summary(iris[fit == 1,])
```

# kmeans
```{R}
set.seed(22)
iris_s = as.data.frame(scale(iris[,-5]))
k_model =kmeans(iris_s, centers=3)
?kmeans

barplot(t(k_model$centers), beside =TRUE,xlab="cluster", ylab="value")
?barplot
k_model$centers

summary(iris[k_model$cluster == 1,])
```


```{R}
library('cluster')
set.seed(22)
k_model =kmeans(iris_s, 3)
kms=silhouette(k_model$cluster,dist(iris_s))
summary(kms)
plot(kms)

## find k
#install.packages("fpc")
library(fpc)
?cluster.stats
c_stats = cluster.stats(dist(iris_s),k_model$cluster)
c_stats$avg.silwidth
c_stats$within.cluster.ss

nk=2:6
cluster.stats(dist(iris_s),kmeans(iris_s,centers=2)$cluster)
cluster.stats(dist(iris_s),kmeans(iris_s,centers=2)$cluster)$avg.silwidth
cluster.stats(dist(iris_s),kmeans(iris_s,centers=2)$cluster)$within.cluster.ss

# avg silhouette
SW = sapply(nk,function(k){
  set.seed(1309); cluster.stats(dist(iris_s),kmeans(iris_s,centers=k)$cluster)$avg.silwidth
})

plot(x=nk,y=SW,type='l')

#within sum of squares
WSS =sapply(nk, function(k){set.seed(22);cluster.stats(dist(iris_s),kmeans(iris_s,k)$cluster)$within.cluster.ss})
WSS
plot(x=nk, y=WSS, type="l", xlab="number of k", ylab="within sum of squares")
```

```{R}
setwd('~/lecture/riii')
customer=read.csv('./data/customer.csv',header = T) 

head(customer)
str(customer)

customer_s =scale(customer[,-1])

nk= 2:8
SW = sapply(nk,function(k){ set.seed(22);
cluster.stats(dist(customer_s),kmeans(customer_s,k)$cluster)$avg.silwidth })

WSS = sapply(nk, function(k){ set.seed(22);
cluster.stats(dist(customer_s),kmeans(customer_s,k)$cluster) $within.cluster.ss
})

plot(x=nk,y=SW,type='l')
plot(x=nk,y=WSS,type='l')
k_model = kmeans(customer_s,centers=4)

single_c = hclust(dist(customer_s), method="single")
hc_single = cutree(single_c, k = 4)
complete_c = hclust(dist(customer_s), method="complete")
hc_complete = cutree(complete_c, k = 4)

sapply(
list(kmeans = k_model$cluster,hc_single =
hc_single,hc_complete = hc_complete), function(c) { cluster.stats(dist(customer_s), c)[c("avg.silwidth")] })

```

### density-based method-DBSCAN
- http://123android.blogspot.tw/2012/01/28dec11-data-mining.html
```{R}
#install.packages("mlbench")
# mlbench package provides many methods to generate simulated data with different shapes and sizes.
#In this example, we generate a Cassini problem graph
library(mlbench)
#install.packages("fpc")
library(fpc)
set.seed(2)
p = mlbench.cassini(500)
plot(p$x,col=p$classes)

?mlbench.cassini

ds = dbscan(data = dist(p$x),eps= 0.2, MinPts = 5, method="dist")
ds
plot(ds, p$x)

#filter群集的raw data
cluster_1_raw = p$x[ds$cluster == 1,]
cluster_1_raw
```
